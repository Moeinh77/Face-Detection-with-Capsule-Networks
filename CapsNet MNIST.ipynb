{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CapsNet MNIST\n",
    "\n",
    "This is an implementation of Geoffrey Hinton's CapsNet architecture as described in [Sabour 2017]. The implementation is (very) insipired by Aurélien Géron's implementation and is for learning purposes only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# TODO adjust matplotlib settings for Jupyter Notebooks\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stabilize ouptut accross runs\n",
    "# TODO give the caller more control over what to reset\n",
    "\n",
    "def reset(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-afdce078956b>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# TODO this method of loading mnist appears to be deprecated;\n",
    "#     => error message: \"Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\"\n",
    "# TODO switch to `tf.data.Dataset`\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def peek_data(n_samples=5):\n",
    "    plt.figure(figsize=(n_samples * 2, 3))\n",
    "    \n",
    "    for index in range(n_samples):\n",
    "        plt.subplot(1, n_samples, index + 1)\n",
    "        \n",
    "        # images are stored as flattened numpy arrays\n",
    "        sample_image = mnist.train.images[index].reshape(28, 28)\n",
    "        \n",
    "        plt.imshow(sample_image, cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def peek_labels(n_samples=5):\n",
    "    print(mnist.train.labels[:n_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peek_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 3 4 6 1]\n"
     ]
    }
   ],
   "source": [
    "peek_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CapsNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset()\n",
    "\n",
    "## Input\n",
    "\n",
    "# tf.layers.conv2d expects tensors of shape (batch_size, width, height, channels)\n",
    "X = tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.float32, name=\"X\")\n",
    "y = tf.placeholder(shape=[None], dtype=tf.int64, name=\"y\")\n",
    "\n",
    "\n",
    "## Conv1\n",
    "\n",
    "# The Conv1 layer is purely convolutional, with 256 9x9 kernels, a stride of 1 and ReLU activations. Its purpose is \n",
    "# to extract visual features from pixels. Its output will be a tensor of shape (batch_size, 20, 20, 256)\n",
    "conv1_params = {\n",
    "    \"filters\": 256,\n",
    "    \"kernel_size\": 9,\n",
    "    \"strides\": 1,\n",
    "    \"padding\": \"valid\", # don't use zero-padding\n",
    "    \"activation\": tf.nn.relu,\n",
    "}\n",
    "\n",
    "conv1 = tf.layers.conv2d(X, name=\"conv1\", **conv1_params)\n",
    "\n",
    "\n",
    "## PrimaryCaps\n",
    "# TODO make this a layer, similar to tf.layers.conv2d\n",
    "\n",
    "# In the 2017 paper, the probability that the object detected by the capsule is present has been\n",
    "# represented by the length of the capsules activation vector. Hinton backtracked on this in his\n",
    "# recent paper \"Matrix Capsules with EM Routing\", since the use of the `squash` non-linearity \n",
    "# is unprincipled and brings some numerical issues.\n",
    "\n",
    "# TODO try the Hinton 2018 capsule design\n",
    "\n",
    "# a numerically stable implementation of the squash function\n",
    "def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
    "    with tf.name_scope(name, default_name=\"squash\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis, keepdims=True)\n",
    "        \n",
    "        # approximate norm(s) by sqrt(sum_i s_i^2 + epsilon) to prevent division by zero\n",
    "        safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "        \n",
    "        squash_factor = squared_norm / (1. + squared_norm)\n",
    "        unit_vector = s / safe_norm\n",
    "        \n",
    "        return squash_factor * unit_vector\n",
    "\n",
    "    \n",
    "with tf.name_scope('primaryCaps') as scope:\n",
    "    # The PrimaryCaps layer is the first layer of capsules, containing 32 channels of 8D capsules\n",
    "    # The capsule components are computed by applying a convolution (9x9 kernel, stride of 2) on the\n",
    "    # 256 20x20 feature maps of the Conv1 layer. The resulting 256 6x6 feature maps are then grouped\n",
    "    # accross feature maps (ie. accross the `channel` dimension) into 32 6x6 maps of 8D capsules,\n",
    "    # resulting in 1152 primary capsules in total\n",
    "    primaryCaps_n_maps = 32\n",
    "    primaryCaps_n_dims = 8\n",
    "    primaryCaps_n_caps = primaryCaps_n_maps * 6 * 6 # 1152\n",
    "\n",
    "    # The PrimaryCaps layer is a convolutional capsule layer acting on the feature maps of Conv1. \n",
    "    primaryCaps_conv_params = {\n",
    "        \"filters\": primaryCaps_n_maps * primaryCaps_n_dims,\n",
    "        \"kernel_size\": 9,\n",
    "        \"strides\": 2,\n",
    "        \"padding\": \"valid\",\n",
    "        \"activation\": tf.nn.relu,\n",
    "    }\n",
    "\n",
    "    primaryCaps_conv = tf.layers.conv2d(conv1, name=\"primaryCaps_conv\", **primaryCaps_conv_params)\n",
    "\n",
    "    # The proper way would be to reshape the (batch_size, 6, 6, 256) to (batch_size, 6, 6, 32, 8). \n",
    "    # However, since this layer is fully connected to the next layer, Ageron simply reshapes to\n",
    "    # a (quasi) flat tensor of 1152 8D capsules (ie. (batch_size, 6x6x32, 8))\n",
    "\n",
    "    # TODO this approach doesn't support stacking convolutional capsule layers. Make this more general!\n",
    "    primaryCaps_raw = tf.reshape(\n",
    "        primaryCaps_conv, \n",
    "        shape=[-1, primaryCaps_n_caps, primaryCaps_n_dims],\n",
    "        name=\"primaryCaps_raw\"\n",
    "    )\n",
    "    \n",
    "    primaryCaps_output = squash(primaryCaps_raw, name=\"primaryCaps_output\")\n",
    "\n",
    "\n",
    "## DigitCaps\n",
    "# TODO make this a layer, similar to tf.layers.dense\n",
    "\n",
    "# The second layer of capsules contains 10 16D capsules (one per digit class). \n",
    "digitCaps_n_caps = 10\n",
    "digitCaps_n_dims = 16\n",
    "\n",
    "# The PrimaryCaps layer is fully connected\n",
    "# to this layer, ie. every primary capsule i predicts the estimation of every digit capsule j via a transformation \n",
    "# matrix W_ij applied to its own activation (ie. u_pred,j|i = W_ij u_i).\n",
    "\n",
    "# tf.matmul applied to higher-dimensional tensors treats these as arrays of matrices and performs element-wise\n",
    "# matrix multiplication on them. We will use this to efficiently compute a tensor of 16D prediction vectors\n",
    "# (laid out as columns vectors) of dimension (batch_size, primaryCaps_n_caps, digitCaps_n_caps, digitCaps_n_dims, 1).\n",
    "# To this end, we create a tensor of shape \n",
    "# (batch_size, primaryCaps_n_caps, digitCaps_n_caps, digitCaps_n_dims, primaryCaps_n_dims) containing a\n",
    "# (primaryCaps_n_caps, digitCaps_n_caps) array of transformation matrices, replicated accross the `batch_size`\n",
    "# dimension. This tensor is multiplied as outlined above with another tensor of shape \n",
    "# (batch_size, primaryCaps_n_caps, digitCaps_n_caps, primaryCaps_n_dims, 1) representing an array of similar shape\n",
    "# containing the digitCaps_n_dims dimensional activation vectors of the digitCaps layer replicated accross\n",
    "# the `digitCaps_n_caps` dimension.\n",
    "# I probably won't understand any of this myself when I look at it a second time...\n",
    "\n",
    "# Create a (primaryCaps_n_caps, digitCaps_n_caps) array of transformation matrices\n",
    "sigma = 0.1\n",
    "\n",
    "W = tf.Variable(\n",
    "    tf.random_normal(\n",
    "        shape=(1, primaryCaps_n_caps, digitCaps_n_caps, digitCaps_n_dims, primaryCaps_n_dims),\n",
    "        stddev=sigma,\n",
    "        dtype=tf.float32\n",
    "    ),\n",
    "    name = \"W\",\n",
    ")\n",
    "\n",
    "# Replicate the array of transformation matrices accross the `batch_size` dimension\n",
    "batch_size = tf.shape(X)[0]\n",
    "W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")\n",
    "\n",
    "# Convert the output of the PrimaryCaps layer into column vectors\n",
    "primaryCaps_output_columns = tf.expand_dims(primaryCaps_output, -1, name=\"primaryCaps_output_columns\")\n",
    "\n",
    "# Add the `primaryCaps_n_caps` dimension\n",
    "primaryCaps_output_tile = tf.expand_dims(primaryCaps_output_columns, 2, name=\"primaryCaps_output_tile\")\n",
    "\n",
    "# Replicate primaryCaps ouptut column vectors accross the `primaryCaps_n_caps` dimension\n",
    "primaryCaps_output_tiled = tf.tile(\n",
    "    primaryCaps_output_tile, \n",
    "    [1, 1, digitCaps_n_caps, 1, 1], \n",
    "    name=\"primaryCaps_output_tiled\"\n",
    ")\n",
    "\n",
    "# Multiply both tensors to get the predictions for the DigitCaps layer\n",
    "digitCaps_predictions = tf.matmul(W_tiled, primaryCaps_output_tiled, name=\"digitCaps_predictions\")\n",
    "\n",
    "## Routing by agreement\n",
    "\n",
    "\n",
    "# This simple form of routing by agreement basically performs coincidence filtering in the prediction space via\n",
    "# K-means clustering with K=1. A more complex and flexible procedure has been outlined by Hinton in his 2018 paper\n",
    "# TODO implement EM routing\n",
    "\n",
    "# Routing in CpasNets works in the following way: a primary capsule sends each digit capsule its respective prediction\n",
    "# vector. This input signal is dampened by a coupling coefficient. The coupling coefficients c_ij connecting a primary\n",
    "# capsule i to a digit capsule j represent the probabilitic beliefs that the object detected by capsule i is part\n",
    "# of the object detected by capsule j. Therefore, the c_ij's have to sum to one over all digit capsules for a\n",
    "# particular primary capsule. This is modeled by a set of log priors b_ij which are iteratively tuned during top-down\n",
    "# feedback\n",
    "\n",
    "\n",
    "### Round 1\n",
    "# This implementation explicitly specifies two rounds of routing. This is not only ugly, it also doesn't scale\n",
    "# to a higher number of rounds.\n",
    "# TODO put this in a look and make the number of iterations a hyperparamter\n",
    "\n",
    "# Initialize a set of log priors\n",
    "\n",
    "# Note: The idea here is to create a tensor of log_priors which can be multiplied elementwise with the \n",
    "# `digitCaps_predictions` tensor. This will be done using tf.multiply on higher-dimensional tensors as has been\n",
    "# done above when computing the predicitions. Since `digitCaps_predictions` is a \n",
    "# (batch_size, primaryCaps_n_caps, digitCaps_n_caps) array of (digitCaps_n_dims, 1)-shaped column-vectors, and \n",
    "# the log_prior tensor must be of the same rank, `log_priors` will have a seemingly redundant last dimension of size 1.\n",
    "\n",
    "# Note: Those are constants! They will (or should?) not be modified during training. Hinton says that log priors could\n",
    "# be trained discriminatively, but they simply use 0's (ie. equally distributed priors).\n",
    "# TODO: is this right? should they be learned? investiage!\n",
    "log_priors = tf.zeros([batch_size, primaryCaps_n_caps, digitCaps_n_caps, 1, 1], dtype=tf.float32, name=\"log_priors\")\n",
    "\n",
    "\n",
    "# Apply softmax accross the `digitCaps_n_caps` dimension to compute coupling coefficients from log priors \n",
    "coupling_coefficients = tf.nn.softmax(log_priors, axis=2, name=\"coupling_coefficients\")\n",
    "\n",
    "# Weight predictions with coupling coefficients\n",
    "# Since each vector in `digitCaps_predictions` is multiplied with a scalar, the result will again be a tensor of shape\n",
    "# (batch_size, primaryCaps_n_caps, digitCaps_n_caps, digitCaps_n_dims, 1)\n",
    "weighted_predictions = tf.multiply(coupling_coefficients, digitCaps_predictions, name=\"weighted_predictions\")\n",
    "\n",
    "# Accumulate incoming weighted_predictions for each digit capsule, creating a tensor of \n",
    "# shape (batch_size, 1, 10, 16, 1)\n",
    "weighted_sums = tf.reduce_sum(weighted_predictions, axis=1, keepdims=True, name=\"weighted_sums\")\n",
    "\n",
    "# Apply the squash-nonlinearity on each 16D prediction vector to compute the digit capsule's outputs\n",
    "# for the first iteration\n",
    "digitCaps_output_round_1 = squash(weighted_sums, axis=-2, name=\"caps_output_round_1\")\n",
    "\n",
    "### Round 2\n",
    "# Predictions close to the output of a digit capsule (ie. tight clusters in prediction space) should be reinforced\n",
    "# via top-down feedback. In this version of routing by agreement, this is done by computing the \"agreement\" (the \n",
    "# scalar product) between these two vectors, treat the result as a log likelihood and add it to the corresponding\n",
    "# initial log_prior b_i,j. \n",
    "# Hinton acknowledges that this is an \"ugly hack\", but it works and it's fast. For a better (yet, slower) method\n",
    "# consult \"Matrix capsules with EM-Routing\"\n",
    "\n",
    "# Compute the scalar product uhat_j|i * v_j for each pair of primary capsules i and digit capsules j. \n",
    "# We therefore replicate `digitCaps_output_round_1` along the formerly reduced `primaryCaps_n_caps` dimension\n",
    "digitCaps_output_round_1_tiled = tf.tile(\n",
    "    digitCaps_output_round_1,\n",
    "    [1, primaryCaps_n_caps, 1, 1, 1],\n",
    "    name=\"digitCaps_output_round_1_tiled\"\n",
    ")\n",
    "\n",
    "# tf.matmul applied to tensors of rows and column vectors computes the element-wise scalar product of these vectors\n",
    "agreement = tf.matmul(digitCaps_predictions, digitCaps_output_round_1_tiled, transpose_a=True, name=\"agreement\")\n",
    "\n",
    "# update the log_priors\n",
    "log_priors_round_2 = tf.add(log_priors, agreement, name=\"log_priors_round_two\")\n",
    "\n",
    "# the rest is analogous to the computations in round one\n",
    "coupling_coefficients_round_2 = tf.nn.softmax(log_priors_round_2, axis=2, name=\"coupling_coefficients_round_2\")\n",
    "\n",
    "weighted_predictions_round_2 = tf.multiply(\n",
    "    coupling_coefficients_round_2,\n",
    "    digitCaps_predictions,\n",
    "    name=\"weighted_predictions_round_2\"\n",
    ")\n",
    "\n",
    "weighted_sums_round_2 = tf.reduce_sum(\n",
    "    weighted_predictions_round_2,\n",
    "    axis=1,\n",
    "    keepdims=True,\n",
    "    name=\"weighted_sums_round_2\"\n",
    ")\n",
    "\n",
    "digitCaps_output_round_2 = squash(weighted_sums_round_2, axis=-2, name=\"digitCaps_output_round_2\")\n",
    "digitCaps_output = digitCaps_output_round_2\n",
    "\n",
    "\n",
    "# Predictions\n",
    "\n",
    "# the probability that a digit is present is encoded by the length of the corresponding digit capsule. As described \n",
    "# above, using `tf.norm()` comes with some numerical issues, therefore we implement it ourself:\n",
    "def safe_norm(s, axis=-1, epsilon=1e-7, keepdims=False, name=None):\n",
    "    with tf.name_scope(name, default_name=\"safe_norm\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis, keepdims=keepdims)\n",
    "        return tf.sqrt(squared_norm + epsilon) # prevents norm(s) from becoming zero\n",
    "    \n",
    "    \n",
    "# Compute the length of all 16D capsule vectors\n",
    "y_proba = safe_norm(digitCaps_output, axis=-2, name=\"y_proba\")\n",
    "\n",
    "# Select the digit with the highest estimated probability \n",
    "# Note: This will only yield an index to that digit, ie. a tensor of shape (batch_size, 1, 1)\n",
    "y_proba_argmax = tf.argmax(y_proba, axis=2, name=\"y_proba\")\n",
    "\n",
    "# Convert index to class predictions\n",
    "y_pred = tf.squeeze(y_proba_argmax, axis=[1, 2], name=\"y_pred\")\n",
    "\n",
    "# Classification accuracy on the sample batch\n",
    "correct = tf.equal(y, y_pred, name=\"correct\")\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Margin Loss\n",
    "\n",
    "# In the 2017 paper Hinton uses a margin loss that punishes the deviation of the capsule vectors' lengths compared to\n",
    "# an upper margen m+ (for the capsule corresponding to the expected digit) and a lower margin m- (for the other \n",
    "# capsules, respectively). The latter is down-weighted by a factor lambda to prevent shrinking the lengths of all\n",
    "# vectors during the beginning of the learning phase.\n",
    "\n",
    "m_plus = 0.9\n",
    "m_minus = 0.1\n",
    "lambda_ = 0.5\n",
    "\n",
    "# Convert labels into one-hot encoding\n",
    "T = tf.one_hot(y, depth=digitCaps_n_caps, name=\"T\")\n",
    "\n",
    "# TODO: isn't this the same as y_proba\n",
    "digitCaps_output_norm = safe_norm(digitCaps_output, axis=-2, name=\"digitCaps_output_norm\")\n",
    "\n",
    "# If the capsule vector corresponds to the present digit, the margin loss is max(0, m+ - norm(vec))^2\n",
    "present_error_raw = tf.square(tf.maximum(0., m_plus - digitCaps_output_norm), name=\"present_error_raw\")\n",
    "# TODO why does Geron use tf.squeeze above and tf.reshape here?\n",
    "present_error = tf.reshape(present_error_raw, shape=(-1, 10), name=\"present_error\")\n",
    "\n",
    "# If the capsule vector corresponds to an absent digit, the margin loss is max(0, norm(vec) - m-)^2\n",
    "absent_error_raw = tf.square(tf.maximum(0., digitCaps_output_norm - m_minus))\n",
    "absent_error = tf.reshape(absent_error_raw, shape=(-1, 10), name=\"absent_error\")\n",
    "\n",
    "# Compute the loss for each instance and digit in the batch (this yields a tensor containing the L_k's in the paper)\n",
    "L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error, name=\"L\")\n",
    "\n",
    "# loss for the entire batch is the mean over the sample losses\n",
    "margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name=\"margin_loss\")\n",
    "\n",
    "## Reconstruction Loss\n",
    "\n",
    "# A reconstruction loss is added to the margin loss in order to encourage the digit capsules to preserve and encode\n",
    "# the instantiation parameters of the input digit. The reconstruction loss is computed by feeding the output of the \n",
    "# correct capsule into a decoder and compute the sum of swaured differences between the reconstruction and the original\n",
    "# image. This serves as a regularizer as it prevents the capsules from learning parameters that do not generalize.\n",
    "\n",
    "# Select the respective digit capsule vector. This differs between training and testing; during trianing the capsule\n",
    "# corresponding to the correct digit must be selected vs. during testing the longest output vector must be selected\n",
    "\n",
    "# flag to indicate whether to mask the vector corresponding to the correct label (True) or the predicted label (False)\n",
    "mask_with_labels = tf.placeholder_with_default(False, shape=(), name=\"mask_with_labels\")\n",
    "\n",
    "# select reconstruction targets accordingly\n",
    "# Note: see concerns about tf.cond() in Geron's notebook\n",
    "reconstruction_targets = tf.cond(mask_with_labels, #condition\n",
    "                                 lambda: y, # If True\n",
    "                                 lambda: y_pred, # If False\n",
    "                                 name=\"reconstruction_targets\")\n",
    "\n",
    "reconstruction_mask = tf.one_hot(reconstruction_targets, depth=digitCaps_n_caps, name=\"reconstruction_mask\")\n",
    "\n",
    "# The reconstruction mask is of shape (batch_size, 10). To apply it to the output of the DigitCaps layer, we have\n",
    "# to reshape it accordingly. namely into a (batch_size, 1, 10, 16, 1) tensor\n",
    "# TODO: This is stupid. The last layer of a capsnet should return a sensible tensor\n",
    "reconstruction_mask_reshaped = tf.reshape(\n",
    "    reconstruction_mask, \n",
    "    [-1, 1, digitCaps_n_caps, 1, 1], \n",
    "    name=\"reconstruction_mask_reshaped\"\n",
    ")\n",
    "\n",
    "# apply the mask\n",
    "digitCaps_output_masked = tf.multiply(digitCaps_output, reconstruction_mask_reshaped, name=\"digitCaps_output_masked\")\n",
    "\n",
    "### Decoder\n",
    "# The decoder is a three-layer FCNN with ReLU hidden units and sigmoidal ouptut units.\n",
    "\n",
    "# Flatten the masked output to serve as input for the decoder\n",
    "decoder_input = tf.reshape(digitCaps_output_masked, [-1, digitCaps_n_caps * digitCaps_n_dims], name=\"decoder_input\")\n",
    "\n",
    "# Hyperparameters of the decoder network\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 1024 \n",
    "n_output = 28 * 28\n",
    "\n",
    "# Decoder Network\n",
    "with tf.name_scope(\"decoder\"):\n",
    "    hidden1 = tf.layers.dense(decoder_input, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    decoder_output = tf.layers.dense(hidden2, n_output, activation=tf.nn.sigmoid, name=\"decoder_output\")\n",
    "    \n",
    "### Reconstruction Loss\n",
    "\n",
    "# flatten input image\n",
    "X_flat = tf.reshape(X, [-1, n_output], name=\"X_flat\")\n",
    "squared_difference = tf.square(X_flat - decoder_output, name=\"squared_difference\")\n",
    "reconstruction_loss = tf.reduce_mean(squared_difference, name=\"reconstruction_loss\")\n",
    "\n",
    "## Final Loss Function\n",
    "# The final loss is the sum of the margin loss and the reconstruction loss, dampened by a factor of alpha\n",
    "alpha = 0.0005\n",
    "\n",
    "loss = tf.add(margin_loss, alpha * reconstruction_loss, name=\"loss\")\n",
    "\n",
    "# Log graph for visualization in TensorBoard\n",
    "writer = tf.summary.FileWriter('logs')\n",
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Val accuracy: 20.0000% Loss: 1.690412 (improved)\n",
      "Epoch: 2 Val accuracy: 16.0000% Loss: 0.621719 (improved)\n",
      "Epoch: 3 Val accuracy: 12.0000% Loss: 0.545756 (improved)\n",
      "Epoch: 4 Val accuracy: 34.0000% Loss: 0.478556 (improved)\n",
      "Epoch: 5 Val accuracy: 44.0000% Loss: 0.428978 (improved)\n",
      "Epoch: 6 Val accuracy: 50.0000% Loss: 0.379999 (improved)\n",
      "Epoch: 7 Val accuracy: 58.0000% Loss: 0.363172 (improved)\n",
      "Epoch: 8 Val accuracy: 58.0000% Loss: 0.334208 (improved)\n",
      "Epoch: 9 Val accuracy: 60.0000% Loss: 0.327629 (improved)\n",
      "Epoch: 10 Val accuracy: 60.0000% Loss: 0.320690 (improved)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op  = optimizer.minimize(loss, name=\"training_op\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "## Currently, we only try to overfit on a single sample\n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "checkpoint_path = \"./capsnet_save\"\n",
    "\n",
    "X_train = mnist.train.images[:100]\n",
    "y_train = mnist.train.labels[:100]\n",
    "\n",
    "X_val = mnist.validation.images[:50]\n",
    "y_val = mnist.validation.labels[:50]\n",
    "\n",
    "X_test = mnist.test.images[:50]\n",
    "y_test = mnist.test.labels[:50]\n",
    "\n",
    "n_iterations_per_epoch = len(X_train) // batch_size\n",
    "n_iterations_validation = len(X_val) // batch_size\n",
    "\n",
    "best_loss_val = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the graph\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(n_iterations_per_epoch):\n",
    "            X_batch, y_batch = X_train[iteration:iteration+batch_size+1], y_train[iteration:iteration+batch_size+1]\n",
    "            \n",
    "            _, loss_train = sess.run(\n",
    "                [training_op, loss],\n",
    "                feed_dict={\n",
    "                    X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                    y: y_batch.reshape([-1]),\n",
    "                    mask_with_labels: True\n",
    "                })\n",
    "        \n",
    "            print(\n",
    "                \"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                    iteration + 1, \n",
    "                    n_iterations_per_epoch, \n",
    "                    (iteration + 1)/n_iterations_per_epoch * 100, \n",
    "                    loss_train\n",
    "                ),\n",
    "                end=\"\"\n",
    "            )\n",
    "        \n",
    "        # Measure validation loss and accuracy at the end of each epoch:\n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        \n",
    "        for iteration in range(n_iterations_validation):\n",
    "            X_batch, y_batch = X_val[iteration:iteration+batch_size+1], y_val[iteration:iteration+batch_size+1]\n",
    "            \n",
    "            loss_val, acc_val = sess.run(\n",
    "                [loss, accuracy],\n",
    "                feed_dict={\n",
    "                    X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                    y: y_batch.reshape([-1]),\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "            \n",
    "            print(\n",
    "                \"\\rEvaluating the model: {}/{} ({:.1f})%\".format(\n",
    "                    iteration + 1,\n",
    "                    n_iterations_validation,\n",
    "                    (iteration + 1) * 100 / n_iterations_validation\n",
    "                ),\n",
    "                end=\" \" * 10\n",
    "            )\n",
    "            \n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        \n",
    "        print(\n",
    "            \"\\rEpoch: {} Val accuracy: {:.4f}% Loss: {:.6f}{}\".format(\n",
    "                epoch + 1,\n",
    "                acc_val * 100,\n",
    "                loss_val,\n",
    "                \" (improved)\" if loss_val < best_loss_val else \"\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Save the model if it improved:\n",
    "        if loss_val < best_loss_val:\n",
    "            save_path = saver.save(sess, checkpoint_path)\n",
    "            best_loss_val = loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./capsnet_save\n",
      "Final test accuracy: 70.0000% Loss: 0.284458\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "n_iterations_test = len(X_test) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    \n",
    "    loss_tests = []\n",
    "    acc_tests = []\n",
    "    \n",
    "    for iteration in range(n_iterations_test):\n",
    "        X_batch, y_batch = X_test[iteration:iteration+batch_size+1], y_test[iteration:iteration+batch_size+1]\n",
    "        \n",
    "        loss_test, acc_test = sess.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                y: y_batch.reshape([-1]),\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        loss_tests.append(loss_test)\n",
    "        acc_tests.append(acc_test)\n",
    "        \n",
    "        print(\n",
    "            \"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                iteration + 1,\n",
    "                n_iterations_test,\n",
    "                (iteration + 1) * 100 / n_iterations_test\n",
    "            ),\n",
    "            end=\" \" * 10\n",
    "        )\n",
    "        \n",
    "    loss_test = np.mean(loss_tests)\n",
    "    acc_test = np.mean(acc_tests)\n",
    "    \n",
    "    print(\"\\rFinal test accuracy: {:.4f}% Loss: {:.6f}\".format(acc_test * 100, loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./capsnet_save\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "\n",
    "n_samples = 5\n",
    "\n",
    "sample_images = X_test[:n_samples].reshape([-1, 28, 28, 1])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    digitCaps_output_value, decoder_output_value, y_pred_value = sess.run(\n",
    "        [digitCaps_output, decoder_output, y_pred],\n",
    "        feed_dict={\n",
    "            X: sample_images,\n",
    "            y: np.array([], dtype=np.int64) # as explained earlier, this will not be used\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAACPCAYAAADeIl6VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAExRJREFUeJzt3XmMVkW6x/FfgYZFECFwEb02RMgAPagQNQ4uYFAZBPEqgstllDheIxhc56oJKq64AC7R67jgdRmRsDSLNBfRKy4DQRQiAopI3HAMomzDVUDZ6v7RTVl17Ld9u+vd+n2/n6STp6jT59Sh+rz99Klzqoy1VgAAAKifRvluAAAAQENGMgUAABCBZAoAACACyRQAAEAEkikAAIAIJFMAAAARSKY8xpi3jTH/kevvRebRl8WF/iwe9GVxoT+rFG0yZYz5yhhzZr7bIUnGmDHGmB+9r13GmP3GmLb5bltDUGB9OcgYs9gY809jzEZjzLPGmJb5bldDUmD92cEYM9cYs8EYY40xnfLdpoakkPpSkowx/26MWW+M2WGMmWOMaZPvNjUkhdafBxhjnqu+Prvkuy2pFG0yVUistfdZa1sc+JL0oKS3rbWb89021FkrSfdKOkJSd0lHSpqQ1xYhxn5JCyRdkO+GII4x5veSnpZ0qaT2knZK+mteG4VoxphTJXXOdzt+S0klU8aY1saYecaYTcaYbdXxvyY262yMed8Y83/GmFf8v2yMMX8wxiypviux0hhzej3aYCRdJunFuLMpbfnqS2vtFGvtAmvtTmvtNkmTJJ2SuTMrTXnsz++stX+VtCyDp1PS8vg5O1xSpbX279baHyXdLmkId47j5PP3pjHmIEmPS7omM2eTPSWVTKnqfJ+X1FFSmaRdkv4rsc1lkv4sqYOkvZIekyRjzJGS/kdVdyXaSPpPSTONMe2SBzHGlFX/4JTV0IbTJP2LpJmZOKESVgh9KUl9JH0cfTYolP5EvHz15e8lrTxQb639XNJuSb/L2JmVpnxemzdI+ru1dlVGzygLSiqZstZusdbOrL6r8IOkcZL6JjZ7yVr7kbV2h6r+srnQGNNY0p8kzbfWzrfW7rfW/q+k5ZIG1nCcr621h1lrv66hGSMkVVT/5YR6KoS+NMacpar+HJvh0ys5hdCfyIw89mULSdsTm22XxJ2pCPnqT2PMUZKuUgP5fD0o3w3IJWNMc0mPSBogqXX1P7c0xjS21u6rLv/D+5b1kg6W1FZVWfkwY8xgr/5gSW/V8fjDJP1b/c4ABxRAX/5B0hRJQ6216+p3Fjgg3/2JzMljX/4o6dDEvx0q6Ye6nQF8eezPRyXdba1NJsgFqaSSKUl/kdRV0knW2o3GmJ6SVkgy3jZHeXGZpD2SNqvqh+Ula+2VEcc/X9JWSW9H7ANV8taXxphekuZK+rO1dmF99oFfyfe1iczJV19+LOm4AwVjzNGSmkjij504+erPMySdaowZ7/3bu8aY66y1U+qxv6wq9mG+g40xTQ98qSqr3iXpn9UPyN1Rw/f8yRhTXp2N362qIbl9kiZLGmyM+aMxpnH1Pk+v4UG82oyQ9DdrrY08r1JUEH1pjOmhqre/rrHWVmbs7EpPQfSnJFUfv0l1sUl1GekrlL58ufp7TzPGHFK931nVQ1NIX6H05+9UlRz3rP6SpMGSZkeeX1YUezI1X1U/BAe+DpPUTFUZ81JV/VJMeknSC5I2Smoq6VpJstb+Q1XDc2MkbVJVxn2Tavg/rH6Q7kf/QbrqB/H6SfpbZk6t5BRKX/5FUjtJ/21+mTeMB9DrrlD6U9XHP/AM49rqMtJXEH1prf1Y0khVJVXfq+pZqaszdI6lpFD683tr7cYDX9WbbbbWFuT1abhJAgAAUH/FfmcKAAAgq0imAAAAIpBMAQAARCCZAgAAiEAyBQAAECHXk3by6mD+md/eJC30Zf5lqi8l+rMQcG0WD67N4vKb/cmdKQAAgAgkUwAAABFIpgAAACKQTAEAAEQgmQIAAIhAMgUAABCBZAoAACACyRQAAECEXE/aCWTcxIkTXbxr166gbtWqVS6uqKhIuY9Ro0a5uHfv3kHdpZdeGttEAEAR484UAABABJIpAACACCRTAAAAEYy1OV1DkQUb86/BL6Z60UUXBeUZM2ZkdP9dunQJym+88YaLy8rKMnqsSCymmoZ169YF5a5du7r4scceC+quueaanLQphQZ/bdbFjh07XHzTTTe5+Kmnngq2O+GEE1ycvNY7duyYpdZF49osLix0DAAAkE0kUwAAABGYGgENgj+0V5dhvW7durl4wIABLv7iiy+C7ebOnevizz77LKibPHmyi8eMGZP2sVEYVqxYEZQbNfrlb8gjjzwy181BtQ0bNrh40qRJLm7cuHGw3fLly11cWVkZ1I0ePTpLrUNNPvjgAxcPGTIkqPvqq6+yeuzXX3/dxd27dw/qjjrqqKweOx3cmQIAAIhAMgUAABCBZAoAACACz0yhIPnPSUjS7NmzU27bo0cPF/vPPklS27ZtXdyiRQsX7969O9jupJNOcvHKlSuDui1btqTRYhSqDz/8MCj7PwfJ5z6QPZs2bQrKI0aMyFNLUF+vvfaai3/++eecHtv/bH/uueeCuqlTp+a0LTXhzhQAAEAEkikAAIAIDX6Yr6KiwsX+67WSdMQRR7i4adOmQd3w4cNdfPjhhwd1yRmwkXvffvttUPZn6veH9aTw1nOHDh3S2v/EiROD8ieffJJy23POOSetfaJwrF692sWPP/54UHfZZZflujkly59hfs6cOUHdsmXL6ry/RYsWBWX/c+G4444L6vr06VPn/SO0d+/eoDx//vw8tSScCf/hhx8O6vzZ9A855JCctcnHnSkAAIAIJFMAAAARSKYAAAAiNPhnpvzVxusynb2/Mvmhhx4a1JWXl0e3K13JafBvvvlmF/tjxKVm8ODBQdlf4qVly5ZBXZs2beq8/2nTpgXl5FQJaNg+/fRTF/vPU0jh0kTIruuvv97FyWVi6mPWrFkpy2VlZUHd9OnTXXz88cdHH7sUvfXWW0F5yZIlLr7lllty2patW7e6+OOPPw7qdu7c6WKemQIAAGiASKYAAAAiNPhhvmeffdbFyZmr/eG6NWvWBHX+SvJvv/12ULd06VIX+7eOv/7667TbdfDBB7vYn4VbCl/7948lhcN+pTzMl9SxY8fofUyYMMHF69atS7mdPxt6TWUUvvHjx7u4U6dOQR3XVfYMHDgwKPtTF+zbt69e+/Q/P5NDOOvXr3fxl19+GdSdeOKJLt6/f3+9jl2K/GlFLr744qDOnzZozJgxOWuT9OvVLQoNd6YAAAAikEwBAABEIJkCAACI0OCfmTrjjDNqjJMGDBiQsm7btm1B2X+eyn++oi7LHzRp0sTFXbt2Deq6devmYv91T0nq3Llz2sfAb5s3b56Lx44d6+Lkiuft27d38QMPPBDUNW/ePEutQ6Ykp0Xxr9Xk9ZevV6eL1TvvvOPitWvXBnXGGBenOzXCyJEjg3L//v1d3KpVq6DuzTffdPG4ceNS7vPJJ58MyqNGjUqrLaXI/3/0pxyQpMmTJ7u4RYsWWW1H8nej/3Pm/1wVCu5MAQAARCCZAgAAiNDgh/kyoXXr1kG5X79+NW5X2zBibWbOnBmU/WHFY489NqhLvoqKOMuXL3dxcmjP58+K3bdv36y2CZnnDwEktWvXLoctKX7JIVX/M2vz5s1p78efdmbo0KEuvuOOO4Ltahtm96dMefrpp4M6vy3+yhKS9NNPP7l49OjRQZ0/rU0pqKioCMrz5893sT8VghRON5Ft9957b1D2h/ZOP/30oO6www7LRZNqxZ0pAACACCRTAAAAEUimAAAAIvDMVJZ8//33Lr766quDOn+JBf91fUlq06ZNdhtW5M4777yg/Nprr9W43YgRI4JycnweDcuqVatS1iWfl0GcPXv2BOV0n5Pq06dPUJ42bZqLk0tupct/Ziq5vMmNN97o4h07dgR1/s/EueeeG9SV2vQ0M2bMCMr+/1Wup5Dwn8ebMmVKUHfQQb+kK7fddltQVwjPuXFnCgAAIALJFAAAQASG+bLkiSeecLE/5CeFr3EmZ2dG3X377bcuXrJkSVDnT4fgvyKfvE2c7dl8kXnvvvuui59//vmgrlevXi4+66yzctYmhPxX6ZN9VN+hvVSSw3Uvv/yyi99///2MHquh2759u4uXLl2acrvkIyrZ9swzz7h406ZNQV15ebmLU01flE/cmQIAAIhAMgUAABCBYb4MWbx4cVBOLpbre+WVV1zco0ePrLWpVAwZMsTFtb1ZNHz4cBeX2hs7xWjhwoUuTi5W7i9s3rRp05y1qRTt27cvZd17772Xs3b4b0lL0v79+1PW+W1OzrjuL+ZbrPzHH7755pug7pJLLsl1c5zPP/88ZV2h/67kzhQAAEAEkikAAIAIJFMAAAAReGYqQ/yVtiVp9+7dLj7zzDODut69e+ekTcVq7ty5QXnFihUpt/VXF7/77ruz1STkwcqVK1PWDRs2LIctKS1PPfVUUG7cuHGeWhKqrKwMyv7ngjEmqPPbfNddd2W3YQWoZcuWLu7Zs2dQt3r1ahdv3bo1qMv0Ch3JaYOSs7H7TjnllIweO9O4MwUAABCBZAoAACACw3wRdu3a5eIFCxYEdU2aNHFx8jZyISzK2NBs2bLFxffdd19Q5w+pJvm3sJnlvOHbuHGjixctWuTibt26Bdudf/75OWtTqZk3b17ejp2cFXvNmjUuTn4u1Maffb0UP4+bNWvm4i5dugR1FRUVLh40aFBQ5y8ena6PPvooKPvTH6xfvz6oSw7H+ho1Kux7P4XdOgAAgAJHMgUAABCBZAoAACACz0xFmDBhgouTr+efffbZLj755JNz1qZi9dBDD7m4thXgzzvvvKDMdAjF5YUXXnDxd99952L/ekPxGjduXFB+4okn0vq+Tp06BeUXX3zRxWVlZdHtasjuvPPOoOwvvZN8Pu7iiy+u8/7btWsXlP3nompb/ivp8ssvr/Oxc4k7UwAAABFIpgAAACIwzFcHyVue99xzj4tbtWoV1N1+++05aVOpePjhh9PaLnnbn+kQikvyVeoDWrduneOWIFcGDhzo4rVr19ZrH+Xl5UH5tNNOi2pTMenevXtQnj59uouTj6/40xqka+jQoSnrRowYEZQnT56cclt/OodCxJ0pAACACCRTAAAAEUimAAAAIvDM1G/wlzG59tprg7q9e/e62B/Xl6TevXtnt2Gokd9fUv2Wikg+/+bvY8+ePUHd9u3bU+5n27ZtLn7kkUfSPr6/ov2DDz4Y1DVv3jzt/RSjysrKGv/9nHPOyXFLSpf/6rwk7du3L+W2r776asq6K6+80sUbNmxI63i1LTdSm3wugdOQ9erVq9ZyrKOPPjrtbVevXu3iY445JqPtyATuTAEAAEQgmQIAAIjAMF8N/NvWAwYMcPGXX34ZbOevtu1Pk4D8OfbYY6P3ceGFFwblDh06uNifdVuSpk6dGn282rRv3z4o33bbbVk9XqFZtGhRUE7+/yP3Ro0aFZRvvvnmlNsOGjTIxf7wdVJtdf7ncW3bJY0cOTLtbZEfySHjZNlXiEN7Pu5MAQAARCCZAgAAiEAyBQAAEIFnpmrgT5m/fPnylNv5S5x07tw5q20qdf7UE3PmzMnqsfzlFOoiOQ1Do0ap/1Y599xzXXzCCSek3O7UU0+tV1uKxezZs4OyPx2J/5p23759c9amUjdkyJCgPH78eBdv3rw5q8du27ZtUPaXQpk0aVJQ5z/riMKUnOqivlNfFALuTAEAAEQgmQIAAIjAMJ9+vRJ9//79a9xu4sSJQZlZl3Nn1qxZLvaHFSRp9+7dae1jzZo1Lq7LlAZXXHGFizt27JhyuwsuuCAoJ1djR3p27tzp4tpm0B42bJiL6/LKPOIkr4Fp06a5ODkE/+ijj2b02LfeemtQHj16dEb3j9z66aefUtY1a9Yshy2Jx50pAACACCRTAAAAEUimAAAAIpjapm/PgpweLF1jxowJyvfff3+N2y1btiwo1/ZKewHL1LunBdmXJSaT7xEXTH/u2bPHxX369Anq/OV1pkyZ4uLmzZtnv2HZV3TX5oIFC1z8zDPPBHWVlZUuHjx4sIuvuuqqYDv/d1R5eXlQV1ZWlpF2ZkFRXpuZdvjhhwdl/9ofO3ZsUHfdddflpE0p/GZ/cmcKAAAgAskUAABAhJId5vNXo/dXNpekH374ocbvYZgvUDB9WcIYSiguXJvFg2szDf7wriTdcMMNLu7Xr1+um1MbhvkAAACyiWQKAAAgAskUAABAhJJdTmbx4sUuTvWMlCR16dLFxS1atMhqmwAAKBX+9BgNHXemAAAAIpBMAQAARCjZYb7a9OzZ08ULFy50cZs2bfLRHAAAUMC4MwUAABCBZAoAACACyRQAAECEkl1OpoSxZEXxYMmK4sK1WTy4NosLy8kAAABkE8kUAABAhFwP8wEAABQV7kwBAABEIJkCAACIQDIFAAAQgWQKAAAgAskUAABABJIpAACACCRTAAAAEUimAAAAIpBMAQAARCCZAgAAiEAyBQAAEIFkCgAAIALJFAAAQASSKQAAgAgkUwAAABFIpgAAACKQTAEAAEQgmQIAAIhAMgUAABCBZAoAACACyRQAAEAEkikAAIAIJFMAAAAR/h/GAH4THTWYVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x216 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_images = sample_images.reshape([-1, 28, 28])\n",
    "reconstructions = decoder_output_value.reshape([-1, 28, 28])\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    plt.imshow(sample_images[index], cmap=\"binary\")\n",
    "    plt.title(\"Label:\" + str(y_test[index]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    plt.imshow(reconstructions[index], cmap=\"binary\")\n",
    "    plt.title(\"Predicted:\" + str(y_pred_value[index]))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bachelor-Thesis",
   "language": "python",
   "name": "bachelor-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
