{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "In this notebook, we want to reproduce the MNIST CapsNet from Hinton 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from caps import layers, losses\n",
    "from data import mnist, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorBoard logging\n",
    "ROOT_LOGDIR = \"tf_logs\"\n",
    "\n",
    "def log_run():\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    logdir = \"{}/run-{}\".format(ROOT_LOGDIR, now)\n",
    "    \n",
    "    return logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, val, test = mnist.load_data(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<data.mnist.Dataset at 0x112562f98>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def peek_data(dataset, n_samples=5):\n",
    "    X, y = next(dataset.batch(n_samples))\n",
    "    \n",
    "    plt.figure(figsize=(n_samples * 2, 3))\n",
    "    \n",
    "    for index in range(n_samples):\n",
    "        plt.subplot(1, n_samples, index + 1)\n",
    "        \n",
    "        # images are stored as flattened numpy arrays\n",
    "        sample_image = X[index].reshape(28, 28)\n",
    "        \n",
    "        plt.imshow(sample_image, cmap=\"binary\")\n",
    "        plt.title(y[index])\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-328e7bf68a19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Developer/Bachelor-Thesis/data/utils.py\u001b[0m in \u001b[0;36mpeek\u001b[0;34m(dataset, num_samples)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Bachelor-Thesis-4p2whVQY/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m                         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3204\u001b[0m                         \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3205\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   3206\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Bachelor-Thesis-4p2whVQY/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1853\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1855\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Bachelor-Thesis-4p2whVQY/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5485\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Bachelor-Thesis-4p2whVQY/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    651\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    652\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.peek(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CapsNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "## Inputs\n",
    "\n",
    "X = tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.float32, name=\"X\")\n",
    "y = tf.placeholder(shape=[None], dtype=tf.int64, name=\"y\")\n",
    "y_one_hot = tf.one_hot(y, depth=10, name=\"y_one_hot\")\n",
    "## Model\n",
    "\n",
    "conv1 = tf.layers.conv2d(\n",
    "    X, \n",
    "    filters=256,\n",
    "    kernel_size=9,\n",
    "    strides=1,\n",
    "    padding=\"valid\",\n",
    "    name=\"conv1\"\n",
    ")\n",
    "\n",
    "primaryCaps = layers.primaryCaps(\n",
    "    conv1, \n",
    "    caps=32, \n",
    "    dims=8,\n",
    "    kernel_size=9,\n",
    "    strides=2,\n",
    "    name=\"primaryCaps\"\n",
    ")\n",
    "\n",
    "digitCaps = layers.denseCaps(\n",
    "    primaryCaps, \n",
    "    caps=10, \n",
    "    dims=16\n",
    ")\n",
    "\n",
    "\n",
    "probabilities = layers.norm(digitCaps, axis=-1, name=\"probabilities\")\n",
    "margin_loss = losses.margin_loss(y_one_hot, probabilities, name=\"margin_loss\")\n",
    "\n",
    "\n",
    "predictions = tf.argmax(probabilities, axis=-1, name=\"predictions\")\n",
    "with tf.variable_scope(\"accuracy\"):\n",
    "    correct = tf.equal(y, predictions, name=\"correct\")\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "\n",
    "with tf.variable_scope(\"reconstructions\"):\n",
    "    mask_with_labels = tf.placeholder_with_default(False, shape=(), name=\"mask_with_labels\")\n",
    "\n",
    "    reconstruction_targets = tf.cond(mask_with_labels,\n",
    "                                     lambda: y,\n",
    "                                     lambda: predictions,\n",
    "                                     name=\"reconstruction_targets\")\n",
    "\n",
    "    reconstructions = layers.denseDecoder(digitCaps, reconstruction_targets, [512, 1024, 28*28], name=\"reconstructions\")\n",
    "    \n",
    "reconstruction_loss = losses.reconstruction_loss(X, reconstructions, name='reconstruction_loss')\n",
    "\n",
    "alpha = tf.constant(0.0005, name='alpha')\n",
    "loss = margin_loss + alpha * reconstruction_loss\n",
    "\n",
    "train_margin_loss_summary = tf.summary.scalar('train_margin_loss', margin_loss)\n",
    "train_reconstruction_loss_summary = tf.summary.scalar('train_reconstruction_loss', reconstruction_loss)\n",
    "train_loss_summrary = tf.summary.scalar('train_loss', loss)\n",
    "train_accuracy = tf.summary.scalar('train_accuracy', accuracy)\n",
    "train_summaries = tf.summary.merge(\n",
    "    [train_margin_loss_summary, train_reconstruction_loss_summary, train_loss_summrary, train_accuracy]\n",
    ")\n",
    "\n",
    "val_margin_loss_summary = tf.summary.scalar('val_margin_loss', margin_loss)\n",
    "val_reconstruction_loss_summary = tf.summary.scalar('val_reconstruction_loss', reconstruction_loss)\n",
    "val_loss_summrary = tf.summary.scalar('val_loss', loss)\n",
    "val_accuracy = tf.summary.scalar('val_accuracy', accuracy)\n",
    "val_summaries = tf.summary.merge(\n",
    "    [val_margin_loss_summary, val_reconstruction_loss_summary, val_loss_summrary, val_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/2]\n",
      "Train | Average Accuracy: 97.2949% Loss: 0.042551                              \n",
      "Validation | Iteration 205/208 (98.6%) | Batch Accuracy: 100.0000% Loss: 0.028537HERE\n",
      "Validation | Average Accuracy: 97.0295% Loss: 0.042334                              \n",
      "[Epoch 2/2]\n",
      "Train | Average Accuracy: 98.3766% Loss: 0.026235                              \n",
      "Validation | Iteration 205/208 (98.6%) | Batch Accuracy: 100.0000% Loss: 0.025990HERE\n",
      "Validation | Average Accuracy: 97.2687% Loss: 0.039177                              \n"
     ]
    }
   ],
   "source": [
    "logdir = log_run()\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_op  = optimizer.minimize(loss, name=\"training_op\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 24\n",
    "checkpoint_path = \"./capsnet_mnist_save\"\n",
    "\n",
    "\n",
    "train_iterations_per_epoch = len(X_train) // batch_size\n",
    "val_iterations_per_epoch = len(X_val) // batch_size\n",
    "\n",
    "best_val_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('[Epoch {}/{}]'.format(epoch + 1, num_epochs))\n",
    "        \n",
    "        # TRAIN\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        \n",
    "        for iteration, (X_batch, y_batch) in enumerate(batch(X_train, y_train, batch_size=batch_size)):\n",
    "            \n",
    "            train_op.run(\n",
    "                feed_dict={\n",
    "                    X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                    y: y_batch.reshape([-1]),\n",
    "                    mask_with_labels: True\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            train_loss, train_accuracy = sess.run(\n",
    "                [loss, accuracy],\n",
    "                feed_dict={\n",
    "                    X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                    y: y_batch.reshape([-1]),\n",
    "                    mask_with_labels: True\n",
    "                }\n",
    "            )\n",
    "        \n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "        \n",
    "            if (iteration + 1) % 10 == 0:\n",
    "                print('\\rTrain | Iteration {}/{} ({:.1f}%) | Batch Accuracy: {:.4f}% Loss: {:.6f}'.format(\n",
    "                    iteration + 1,\n",
    "                    train_iterations_per_epoch,\n",
    "                    (iteration + 1) * 100 / train_iterations_per_epoch,\n",
    "                    train_accuracy * 100,\n",
    "                    train_loss\n",
    "                ), \n",
    "                end=\"\")\n",
    "                \n",
    "                train_summary_str = train_summaries.eval(\n",
    "                    feed_dict={\n",
    "                        X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                        y: y_batch.reshape([-1])\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                file_writer.add_summary(train_summary_str, epoch * train_iterations_per_epoch + iteration)\n",
    "            \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_train_accuracy = np.mean(train_accuracies)\n",
    "        \n",
    "        print('\\rTrain | Average Accuracy: {:.4f}% Loss: {:.6f}'.format(\n",
    "            avg_train_accuracy * 100,\n",
    "            avg_train_loss), \n",
    "        end=\" \" * 30 + \"\\n\")\n",
    "                \n",
    "        \n",
    "        # VALIDATE\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        \n",
    "        for iteration, (X_batch, y_batch) in enumerate(batch(X_val, y_val, batch_size=batch_size)):\n",
    "            \n",
    "            val_loss, val_accuracy = sess.run(\n",
    "                [loss, accuracy],\n",
    "                feed_dict={\n",
    "                    X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                    y: y_batch.reshape([-1]),\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            \n",
    "            if (iteration + 1) % 5 == 0:\n",
    "                print('\\rValidation | Iteration {}/{} ({:.1f}%) | Batch Accuracy: {:.4f}% Loss: {:.6f}'.format(\n",
    "                    iteration + 1,\n",
    "                    val_iterations_per_epoch,\n",
    "                    (iteration + 1) * 100 / val_iterations_per_epoch,\n",
    "                    val_accuracy * 100,\n",
    "                    val_loss\n",
    "                ), \n",
    "                end=\"\")\n",
    "            \n",
    "         \n",
    "                val_summary_str = val_summaries.eval(\n",
    "                    feed_dict={\n",
    "                        X: X_batch.reshape([-1, 28, 28, 1]), \n",
    "                        y: y_batch.reshape([-1])\n",
    "                    }\n",
    "                )\n",
    "                file_writer.add_summary(val_summary_str, epoch * val_iterations_per_epoch + iteration)\n",
    "            \n",
    "\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        avg_val_accuracy = np.mean(val_accuracies)\n",
    "        \n",
    "        print('\\rValidation | Average Accuracy: {:.4f}% Loss: {:.6f}'.format(\n",
    "            avg_val_accuracy * 100,\n",
    "            avg_val_loss), \n",
    "        end=\" \" * 30 + \"\\n\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            best_val_loss = avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./capsnet_mnist_save\n",
      "Evaluation | Accuracy: 90.8973% Loss: 0.113210                       \n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "\n",
    "test_iterations = len(X_test) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    \n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for iteration, (X_batch, y_batch) in enumerate(batch(X_test, y_test, batch_size)):\n",
    "        \n",
    "        test_loss, test_accuracy = sess.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                y: y_batch.reshape([-1]),\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        print(\"\\rEvaluation | Iteration {}/{} ({:.1f}%)\".format(\n",
    "            iteration + 1, \n",
    "            test_iterations, \n",
    "            (iteration + 1) * 100/test_iterations),\n",
    "             end=\" \" * 30\n",
    "        )\n",
    "        \n",
    "    avg_test_loss = np.mean(test_losses)\n",
    "    avg_test_accuracy = np.mean(test_accuracies)\n",
    "    \n",
    "    print(\"\\rEvaluation | Accuracy: {:.4f}% Loss: {:.6f}\".format(avg_test_accuracy * 100, avg_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./capsnet_mnist_save\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "\n",
    "n_samples = 2\n",
    "\n",
    "sample_images = X_train[:n_samples].reshape([-1, 28, 28, 1])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    \n",
    "    digitCaps_output, reconstructions_output, predictions_output = sess.run(\n",
    "        [digitCaps, reconstructions, predictions],\n",
    "        feed_dict={\n",
    "            X: sample_images,\n",
    "            y: np.array([], dtype=np.int64) # will not be used\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAACUCAYAAAB1GVf9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAACuZJREFUeJzt3X2MVNUZx/HfUzUuG5AXeUmxLH80aQUroRqpSBaxgbSKWotgomk0MVb9Q/CFNo2rpEnFQlMjsSEGoUILCMEA2lchsRalqU3FYBMrikZZNVGEFGwQEt5u/9jl6Tlnd2ZnZ2dm7858P8kkz+OZuXOGuz5zzzl37rUsywQAkvSl/u4AgPygIABwFAQAjoIAwFEQADgKAgBHQSjAzHaY2R21fi1qh33cVUMUBDPbZ2Yz+7sfkmRmbWZ2JHgcM7PTZjayv/s2kOVsH882s7+Z2WEz+9TMfm1mQ/q7X6VoiIKQJ1mW/TzLssFnHpJ+IWlHlmUH+7tvqJihkhZLGitpgqQLJP2yX3tUooYtCGY23Mz+aGYHzOxQZ/yV5GlfNbN/mtl/zex3ZjYieP3lZvb3zm+Bf5nZjDL6YJJulfTbvn0adKe/9nGWZRuyLNuWZdnRLMsOSVolaVrlPln1NGxBUMdnXyNpvKQWScckLU+ec6uk2yV9WdJJSb+SJDO7QNKf1PEtMELSjyRtMbNR6ZuYWUvnH1RLN31olTRa0pZKfCB0kYd9LEnTJf27z5+mFrIsq/uHpH2SZvbwnMmSDgX5DklLg3yipOOSzpL0E0nrktdvl3Rb8No7SujX05J+09//PvXwyPE+niXpkKSv9fe/USmPsytYWwYUM2uWtEzSdyUN7/zPQ8zsrCzLTnXmHwUvaZd0jqSR6vjGmWdm1wXt50j6ay/ff56k75X3CdCTHOzjyyVtkDQ3y7K95X2K2mrYgiBpoaSvS/pWlmWfmtlkSbslWfCccUHcIumEpIPq+CNal2XZD/vw/t+X9B91fNOgOvptH5vZNyX9XtLtWZb9pZxt9IdGmkM4x8yazjzU8Y1xTNLhzomkn3bzmh+Y2cTOb5qfSdrc+c2yXtJ1ZvYdMzurc5szupmwKuY2SWuzzuNKVEQu9rGZfUPSNknzsyz7Q8U+XQ00UkH4szr+OM48hkkapI5vg3+oYwem1kn6jaRPJTVJWiBJWZZ9pI5D/TZJB9TxbfJjdfPv2TnhdCSccOqcsPq2pLWV+WjolJd9vFDSKElPB+ebDIhJReMLCsAZjXSEAKAHFAQAjoIAwFEQALhan4fADGY+Wc9P6RX2cz71uJ85QgDgKAgAHAUBgKMgAHAUBACOggDAURAAOAoCAEdBAOAoCAAcBQGAoyAAcBQEAI6CAMBREAA4CgIAR0EA4Br5zk1AF+3t7VG+atWqKH/00Uc97rh59/+ltzSYMGGCx4sXL47a5syZ06d+VgtHCAAcBQGAoyAAcLW+lRtX482nhrrq8oEDB6J8yZIlHj/zzDNR28GDB6M8/P+lpzmEsL2lpSVqe+211zweOXJkKd2uBK66DKB0FAQAjiFDYM2aNVEeHvKdf/75UduePXuifOrUqR63trZWoXdVVddDhnTJb9GiRVEe7udih/1SfOg/atSoou8bDjf27dsXtYVLkm+99VbR7VQQQwYApaMgAHAUBAAut3MIGzZsiPLdu3d7vHr16sr1KHD48OGCbWefHZ/lffz48ShvamryuLm5OWqbNGlSlD/77LMe9zQOrZG6nkO47LLLovz111+P8mJzCBMnTozyHTt2eNzTcuHOnTs9vvLKKwu+56lTp4pup4KYQwBQOgoCAEdBAOByM4fwwAMPRPkTTzwR5adPn65Oj/rBVVdd5fHGjRujtjFjxtS6O1IdziGE54lMmTIlakvPKQnncdJ5gccffzzKw7/Ltra2qC09PTmUns8Q5itWrIja7rzzzoLb6SPmEACUjoIAwOVmyDBu3Lgo//jjj6M8XLobNGhQ2R2YNm2axzfccEPZ20m9+OKLHq9duzZqS09bDYXDB0natGmTxzVckqy7IUPo7bffjvJ0WFBs+XDlypVRfvfdd3u8a9euqO2SSy6J8q1bt3o8d+7cqC0cMuzfv7/k/vQRQwYApaMgAHAUBAAuN3MIe/fujfI333wzymfNmuXxkCFDKtytynr//fejfPbs2VGejmlDjz32mMcLFy6sbMcKq+s5hL4I5wEk6aGHHvJ427ZtRZ+7dOlSjz/77LOobfTo0R6ncwhVxBwCgNJREAA4CgIAl5s5hHq2efPmKJ83b17B54Zr0OnVgauooeYQXnnllSgP53TScwDCS51J0kUXXeRxOA8gdZ0nCM81SLf7wgsveJyev1BFzCEAKB0FAYDjZq9oOOnVuMLTk3u66nLYng4R0teGp57Pnz8/aqvhMKFXOEIA4CgIABwFAYBjDqEKnnzyyShPfyZbzLFjxzxOrw586aWX9q1j6FY6T1Bu2/Tp06M8vNpSXucMUhwhAHAUBACOggDAcepy4JNPPony9evXe7xs2bKyt1Ou8847L8o///zzimy3Gw196vKSJUs8Du/YLHX9qfqRI0c8TucQwrs6SV3nFHKAU5cBlI6CAMA13JAhvDpyuqz31FNPRfkHH3xQkz4Vct9990V5b4YtvdRQQ4beSIcM4RWTnn/++agtXVoMf9FYxSsp9wZDBgCloyAAcBQEAK7uTl1+9913ozy8044kvfTSS2Vtd/z48VE+fPjwgs995JFHorypqSnK77nnHo/feeedgtsZO3Zsb7pY98IrSNXqrlYXXnhhlG/ZssXjq6++OmpLr8IcLlun80F5xRECAEdBAOAoCABcXcwhhOvzy5cvj9rSuygNHjzY46FDh0Zt999/f5SHY/grrrgiakvnFHojfd9QeFeqa6+9tuz3qAfpKcbhnazSsf26detq0qdQW1tblG/fvj3Ki80P5RVHCAAcBQGAq4shw6uvvupxOkS4/vrrozw87KzVr9HeeOONKG9vby/43HPPPdfj9CYhjSBcWrzrrruitjFjxnjcH0MESfriiy88TvtX458BVAVHCAAcBQGAoyAAcHUxh7BixQqPJ02aFLU9/PDDte5OF++9916U79+/v+BzZ86cWe3u5Npzzz3ncbpsN2PGjBr3RtqzZ0+U33jjjR6n/UuvoJQujQ4EHCEAcBQEAK4uhgwjRozwOA9DhFS4LJoaNmxYlC9YsKDa3cm11tZWj9NlvJdfftnj8JeEUtcl2mI3tUmXfXfu3Onx1q1bo7b0qkhhn9IhQvqLxnvvvbdgH/KKIwQAjoIAwFEQALiGu+pyLVx88cVRnl659+TJkx7fdNNNUdumTZuq17HCcnnV5XCJT4rH8+nfbTqeL3Zz1Q8//DDKw5uz9LTdsD2dr0rnf3JypeUQV10GUDoKAgBHQQDgmEOogvCqR1J8g1ApvmJSeHcfSZo6dWr1OlZYLucQwp9CS9I111zj8a5du6K2YmP9Ym1pe3Nzc9SWnt/w4IMPejxnzpyCfc8p5hAAlI6CAMDVxanLebBx40aPjx49GrWlQ4iVK1d63E9DhAEhvRlLOLxatGhR0deGN+5Nly+LLQempxsPxF8s9gVHCAAcBQGAoyAAcCw7lunEiRNRPmXKFI/TU5VvvvnmKF+9enX1OlaeXC47ouJYdgRQOgoCAEdBAOA4D6FM6emwt9xyi8eTJ0+O2mbNmlWTPgF9xRECAEdBAOBYdoTEsmOjYNkRQOkoCAAcBQGAq/WyY6XHqsgn9vMAxRECAEdBAOAoCAAcBQGAoyAAcBQEAI6CAMBREAA4CgIAR0EA4CgIABwFAYCjIABwFAQAjoIAwFEQADgKAgBHQQDgKAgAHAUBgKMgAHAUBACOggDA/Q9AZ6DVt+J2zgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAACUCAYAAAB1GVf9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFXxJREFUeJztnWuMlcd9xp8/9/XCLmsuxhhYGzDYqvElbdXapi2S01p2bLWSlQ9JlFiKEiWf2i9pU1WReo2qSm2/tVKVVqpjy5XaqK3kDzRt1VBZSVzbqmtswDcMGGOMMQZ2Fww2ePrhvAzPPOyZc5bdPeuF5ychzcu8531n5v8y/G8zEyklGGMMAMyZ6QYYYz49eEIwxmQ8IRhjMp4QjDEZTwjGmIwnBGNMxhNCQ0TcGBEpIuY119sj4tEevPcPIuKJ6X6PaWE515l1E0JE7I+IDyNiLCKORMTfR8TiqX5PSumBlNJjXbbns1P9/ubZv9T0k/+kiHhkOt73aeIqk/PyiPhxRByLiBMR8dOIuHc63tWJWTchNDycUloM4DMAfg7Ad7gyWszWvmVSSk+nlBZf+APgIQBjAP5thpvWK64KOaMl068CWAFgCMCfAXjqghbTS2b1YKaUDgHYDuC2iNgREd+NiB8DOA1gfUQMRsTfRcThiDgUEX8SEXMBICLmRsSfR8T7EfEmgM/xs5vnfY2uvx4ReyJiNCJ2R8RnIuJxAOvQEt5YRPxOc+8vRsRPmtn+xYjYRs+5KSL+u3nOfwBYPoEuPwrgBymlU5c1YLOUK13OKaUzKaVXU0qfAAgA59GaGK6dkgGcCCmlWfUHwH4An23KawHsAvDHAHYAeAvAzwCYB2A+gH8B8DcA+gGsBPAsgG80v/0mgFeaZ1wL4EcAEoB5Tf0OAF9ryp8HcAjAz6MlsI0AhrU9zfUNAI4BeBCtCfdXm+sVTf1PAfwlgIUAfhnAKIAn6Pc7AXxxnH73N/dum2kZWM7TI+fm7z5q2ve9GRn3mRb8ZX4oYwBOADgA4K8B9DWC/SO67zoAZwH00d99AcCPmvJ/Afgm1f1a5UP5IYDf6vThNtffBvC43PNDtP53XwfgHIB+qnuSP5RKv78MYB+AmGkZWM7TKudFTfsfnYlx77mNMkX8RkrpP/kvIgIADtJfDaP1v8fhpg5ozeQX7lkt9x+ovG8tgL1dtm0YwOcj4mH6u/lo/c+0GsDxVKr8B5rnd+JRAN9PzVdzlXDVyTmldAbAPzRmy/+llF7ssj1TwmydENrB/1gOovU/x/KU0rlx7j2MUkDrKs89CGBDF++8cO/jKaWv640RMQxgKCL66WNZN84z9HdrAWwD8I3afVcRV6SchfkA1gPo6YQwq52KNVJKhwH8O4C/iIiBiJgTERsi4leaW/4RwG9GxJqIGALwu5XH/S2Ab0XEzzae7Y2N0AHgCFqCu8ATAB6OiPsbh9aiiNgWEWtSSgcAPA/gDyNiQURsBfAwOvNlAD9JKXX7v9dVw5Ug58Y5ubW5ty8ivo2WKfQ/lzMmk+GKnRAavgJgAYDdAI4D+AGA65u676Fl870I4H8B/HO7h6SU/gnAd9GyA0cB/CsueoD/FMB3Gk/zt1JKBwH8OoDfA3AUrf9JfhsXx/qLAH4BwAcAfh/A9/ldEbErIr40Tj86xsqvYma7nBcC+Cu0nJKH0HJUfi6l9M5EB2KyxNVlkhpjalzpGoIxZgJ4QjDGZDwhGGMynhCMMZme5iGMjo4WHsx58y6+fs6cOXpvcb1kyZJc/vjjj4u6uXPn5vLJkyeLur6+vuKa3/nhhx8WdYsXl4vpzp8/n8uffPJJ2+cAwNmzZ3NZHbXnzpXhcW6T3svX+g5tL/dbx4+SdAAAZ86cyeUFCxYUdYODg+XNk+TUqVNFp7idXAYuldfAwEAu1+R84sSJok7lPH/+/FweGxtr+w6glDOXgUvHimWgY6xyXrRoEdrB7+kkZ+6LyllhOfPvAGDp0qUd5WwNwRiT8YRgjMl4QjDGZHrqQ6jZP2p/qT1/6tTFdSL6nOPHj+fyihUrijq1/dnGUhtPn8vvVBtVn8vXartp3z766KO2z2X7Ue3ta665prhmf4P6ItTGXrZsWS6rnTzdcNt4/IHSNwSU9r7a1tyna68ttwrQ/rNPp7+/v6iryVnHWMeq5uPhd+q1Ppe/Af1etL01+NsHSjmrD6YbrCEYYzKeEIwxmZ6uZRgZGSlexiqNqo6qZqv63O45qsqrelgzW1h1BErVTd+vat6xY8dyWcNR+lt+rqqkrGZqW1VW3AZ9h44fP0vrlixZMqVhx5MnTxYNZZmoOlwLLSp8r46xMhE5s9mmctX2sIquddqmWniZ5dzpe+F6bZ+OH/dbv63Fixc77GiM6R5PCMaYjCcEY0ympz6E8+fPFy/jEJSml1533XXF9QcffJDLndI3GQ0Ncars0qVLizq15fhe9XFoG7gvp0+fLuo0hMrvUXue7e1O/eSwqYZQtQ3cvoULFxZ1AwMDU+pDOHfuXCFntpc1JX3VqlXF9dGjR3NZ28k2scpK5czfk8pOfzsyMtL1vTU562/btV2v9R16XZOz/rthn4KmXXcjZ2sIxpiMJwRjTMYTgjEm01MfwrFjx4qXcTqn2k3sMwCA5csvnoSlbeY0UI3hal5CLQVa28Dv0TpNGWXef//94lptYU2vZgYHB3O5tmwaqC9p1r7xszSWvWjRoin1IdTkrO3i/A2gTL1V2BehPoOanGu5DfrbTnLm9r/77rtFncpn5cqVbd/JcubvF7g0V6OW5qzUllX39fXZh2CM6R5PCMaYTE9XO6q6UwuxDQ0NFdccTlGTgVVCTeVUVZJVa71XzRRWH/fs2VPUaciJVcuXX365qFu7tjzBa8uWLbm8evXqoo5Ve+1nLQzZKdTJqqSqqLWdfS6HmpwVXbXIbdP+sgqsKwt1rGpy1l2aeGxUdjpWHDbVe9VEuOuuu3J5zZo1bZ+rIUkdP+5bJznzc7XtatKMhzUEY0zGE4IxJuMJwRiT6akPQcM/E9mRiO1HtbHYftcQn8J2ldpjutvtG2+8kcuc3goATz31VHF96NChXFab9dVXXy2uX3/99Vy+7bbbirqtW7fmstr2tdCijl/NLu0UhpssE9mRqCbn2tLjTn4PlrOm96r/YdeuXbmsfiSV8+HDh9u+k0PjALB378WzeW+//fai7u67785l9QPot8a+LA1Jamiav5Ga76Yd1hCMMRlPCMaYTE9NBlXdOFtLM8RU7WQ1r/Yc3Q1H1UNWCVX909AVhxqffPLJok7NjbfeeiuX161bV9RpthuriC+88EJRx2repk2b2v4OKFdrdgpd9RI1vVg+nWAVWOXM/dVVk7p5K8uZV1DqOwBg3759ufzYY48VdfoeNg01lKiZi2zy1FY7bt68uajjsDRQjp+ao2pG1jbe7QZrCMaYjCcEY0zGE4IxJtNTH4KGTNjG0jRLXaml1wz7CTRUVUs91Xtfeuml4vrZZ5/NZfU3qD3POzzdeOONRZ36Qzh1Vm3Cd955J5fvuOOO6nP44BJdQak2NaeCq59lqqkdgKM+nZqcNYTMvgn1kajfgn08+o5XXnmluH7mmWdyWf0AejAsj+PNN99c1KmfgMOHauvzilgNSep3ORE5s5/lcuRsDcEYk/GEYIzJeEIwxmR66kNQ2J6v7RIDlLZSbXdbTftUe5btPE0p3r17d3HNu/ncf//9Rd3+/fuL63vvvTeX2Q8AXGrfsk2tsWJur8bANS7PS4fV7lQfB/sqNA4/1WhOCbdb5aFtYTlrHxiVs/oJeFw7yZn9QypnTjMHgEceeSSX33vvvaJOl1XX8hC4Tv0A2jf2C9ROAQMmL2drCMaYjCcEY0xmRlOXOaykq8xUXWTVSFVwDkGpCq5pq6zqq0qu97J6qzsbabrp8PDwuG0FgCNHjhTXvIpS+8lmgIa8VF2srexUdXEiB6VOFpUPh9xUHa7JWUOU3G5dyaebtfIYa+q4boLLY647OD300EPFNR8soynZnNYMlN+amkrcbx0DlXPt0NjaYcGXI2drCMaYjCcEY0zGE4IxJtNTH4LaxGzXqt2toRj2E6jdxPakhn40NZhDlpoKrPbjnXfemcuaMqp2MocaNU1V28DjoHYo23033HBDUaeH07IPQd+hOyjVQp1Tje6KxP4MrdNx5PChypnHRv0/2ie2yzXNWW32W2+9NZd1zNXnwaFGbZ+2gb+12tJ1Pdi4Jmf1ndT8SPYhGGMmhScEY0zGE4IxJtNTH0LtxCXdkkxtTUbtKPY3qO2sPoUDBw7k8sGDB4s6tWc5f0D9Cxr3ZruPl9MCl/aNbdr169cXdZzvoHF4tUO5b+qD0d/yO7U9+tvJorY1+wU0F0XlzN+EyoPlrG1WebCcefdj4FL/A+fAqK9I09BZzjt37izqdLkxt1HzVniHZv2e1a/Ey591vGrbBkzk39QFrCEYYzKeEIwxmZ6aDBoG4VBZLUwGlGqohlpqapKmgbIqzTslj9cGDhvpijlWSYHS/NDnqqp/zz33tK3jnXxVdVRVnNvbKcTEob/pTl3WlYes1tZMBKBsp6b78nM1/VhDvSxnNQ1VzmxO6UrIN998s7hmuetqRw1vbtu2LZe1nyznTuYej5nKTs1wHr/a4cDtsIZgjMl4QjDGZDwhGGMyM+pDqO3GW7MJ1Tbia015VtufbSy1JRV+Vm15LVCGttTHoamonLqs93Kdjonu/Mwpr7VDP5VeH/Za211b7W4ec30Ofz/qK9IdrHg8dIct9XFwuJBDfOM9tyZnPeyVfQO15c86JrrzM8tZfQZTLWdrCMaYjCcEY0zGE4IxJtNTH4LaSmzjqI2lvgC2x9TOY1uztmRW6zXOrzYrx5k1jVbj6WwTdtr6jE+H1q3ZuG9ap+nTbPuqn0KXQ7O9O90nQ6ucWQa105iAchxVzmz7a7qvvpPRb0u/Cd76THMC9F5ObdZx1O/ppptuyuXasuqVK1cWdeqL4HRv/QbUz8R99fJnY8yk8IRgjMn01GRQdbGWZqnqGIdXNNTCISgNzWnIidvA6aPApeEoPsxzx44dRZ3upMPP0tDQLbfcUlyzuqvmBPdF1WI1A7i9ncJ5/Ft9bqfw60TRd/P7VI1VWbJ8tI5NJJWzmmncp40bN1bfuWHDhlx+/vnnizpt77Jly9AOXbnK/dYx4b7o91JbwaimdG13Ln1uN6tarSEYYzKeEIwxGU8IxpjMjB72yj4Etd/1mm12tbHYXtQUXvVNsF9An6O2NR/0qXaoLn9mW64WNlI0bMShRrU7tW8catTn6Phx/XQf9qrwOGu7tI+8m5GGkDnUq/axypn9ArXDdoFSzhwqBIC33367uOb2awhQ06n5e9LdlDgMWfP3AKWca6F7fad+z91gDcEYk/GEYIzJeEIwxmR66kNQW47tQLV31H5kW0ltYLaP9RQcXaZ8/fXXj/u78drHOQxqA/JpP0C5TZfat7qTL9uhei+/R1Nh1WblNFW1Q3X82G7uZvfdyVBLI1ZUBrWl0jxWQ0NDRZ36dNgXo7a97rrMPh/192huAfsU1H7XFGRuv/otuA2aK6O5DixnzTuoperXTnVqhzUEY0zGE4IxJjOjOyax6lxbnQeUqqWmqfKqOFXtazvyqOqo6cgcGtKdlHUnptrOuKoScvs1nZRNGv1dLbSodXrNaujl7MY7EdQMqrVTVyLyd6A7UvOBKmreqSpdkzM/ByhNMzVTdNdl7ou2QfvC6ey6GpW/La2rhUk1dVnHk80f77psjJkUnhCMMRlPCMaYTE99CBpaZLtPfQhqh3LoSP0LbBNqaqwe9son/ugSWg057du3L5fVllSbdc+ePbn84IMPFnWaysw7JqkPgUNZndJ8a4fR6pJmHvtOS6Uni4aF+fkaqtOwGctA+3D8+PFc1u9Dd8Xm8KDWqZ+JU5dZ5uPx2muv5bLKWcOF7A/SMa7Z99pv/r7Vr6L38rh02sl8PKwhGGMynhCMMZmemgyqNrF600ltZfW5FlpUNamWpXb06NGi7rnnniuuOftNTRp97n333ZfLfMgncKl6Ozw8nMtqTtQ2Q9XncFhLQ1WqmvOzdPNSDW9OFjXpOAzbqU+s1mq4sPaNqLnH13ooq+6KxIex6DiqnB944IFc3rp1a1Gn4WZeOanZqtwXDVcqq1atyuVOYXW+1jB6N3K2hmCMyXhCMMZkPCEYYzI99SHUUnE72Uac7qv2PKeealqq7oDz9NNP57KmgarNyuml2j5dBcerLHm3HuDS0JCGjhi2qTU0pSE6Du912l2Jf6shyqmmJufaIS76W+0Dr3BUP4jKY/v27bms4W4OXwJleFB9EZs2bWp775YtW4o69dtwX1R2LC8dg9ohNBo6VB8M/1ZXy3aDNQRjTMYTgjEm4wnBGJMJtUGmk9OnTxcvq+26rPYz+w3Uh8DP0TwETlUGyhTkvXv3FnVqW/Kz1q5dW9Spn4BzC3R5tl5zfFhj9mz3qb1YWxZb230XKG1NHb+hoaGJnwpaQeXMbVG5qv3MbVO7m21pHQtNT+Yckp07dxZ16g/iNrGPAAA2b95cXPNOTCpXXcbMfg71I7F/Qb/9mp9Fx0v//XK9PqcbOVtDMMZkPCEYYzI9NRlGR0eLl7FKqKpz7SCOiRyGqasdeXWhrmDUkCWHlTSFVdvHKcidDlNltVhVfVb51GTQMBz/Vt+pKnXtQNf+/v4pNRlGRkaKwZlIijr3qbZDlMpZw5CszqtpqOZFTc5qtugmvox+w/ydqqrP33PtcFygHBNtj47DZOVsDcEYk/GEYIzJeEIwxmRm1IdQC0cpfK/aSewnULtTQ0NsI+ryULUB+Z16MIjaeewX0L7orkhcP5HnqE+hFqKrhad0/BYsWDClPoSxsbG2viJtl17zvXqgDMtZQ6eaDs5yVr9AzTejoUOFfTNq+2t7a7Y/v1PlrGFI/m0nOfP3pMuqFy5caB+CMaZ7PCEYYzKeEIwxmZ76EM6cOVO8rGaPqV2lvoF26HM0TbV2GKa+g+3STst2+VptQIXfo+/k1OVOJwPVlj+r3cx9Vb/FwMDAlPoQzp4921bOOjY6jtwn/Tb5Wv0A6g/i70fHTcecfTwq51qqteaQKPwsfS6nLmudfpfsL1E563Jt7qvKeXBw0D4EY0z3eEIwxmR6umOSql+sGql6qGElVpU0XbMW1lLVsha+1GtWu1XV1WtWH1XVVxWV+1LbXah2UK22QcdL1cXaKrjphvuhctZ0bP5Gamai9re2WlbDgSpnHlcdczULagevapv429OwaC19XZ/L76ylrwOlnLs1s4t3TfgXxpgrFk8IxpiMJwRjTKanYUdjzKcbawjGmIwnBGNMxhOCMSbjCcEYk/GEYIzJeEIwxmQ8IRhjMp4QjDEZTwjGmIwnBGNMxhOCMSbjCcEYk/GEYIzJeEIwxmQ8IRhjMp4QjDEZTwjGmIwnBGNMxhOCMSbjCcEYk/GEYIzJeEIwxmQ8IRhjMv8PN5CUU8rQp8QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 288x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_images = sample_images.reshape([-1, 28, 28])\n",
    "reconstructions_output = reconstructions_output.reshape([-1, 28, 28])\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    plt.imshow(sample_images[index], cmap=\"binary\")\n",
    "    plt.title(\"Label:\" + str(y_test[index]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    plt.imshow(reconstructions_output[index], cmap=\"binary\")\n",
    "    plt.title(\"Predicted:\" + str(predictions_output[index]))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bachelor-Thesis",
   "language": "python",
   "name": "bachelor-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
